{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import nltk,re,pprint\n",
    "import sys,glob,os\n",
    "import operator, string, argparse, math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# class to read and preprocess data\n",
    "class dataProcessor:\n",
    "    def __init__(self, fname, keepFactors):\n",
    "        #keep_factors = ['Job Description', 'Company Name', 'Industry']\n",
    "        self.dataInitial = pd.read_csv(fname, encoding=\"latin\")\n",
    "        if keepFactors:\n",
    "            self.dataInitialSmall = self.dataInitial[['Job Description', 'Company Name', 'Industry']]\n",
    "        else:\n",
    "            self.dataInitialSmall = None\n",
    "\n",
    "    # pipeline for purifying the text, write-pipeline, so just output filename can be provided\n",
    "    def rem_stop_punct(self,originalText, ofilename):\n",
    "        splittedText = originalText.split()\n",
    "        lenl = len(splittedText)\n",
    "        #print(\"Length is: \",lenl, splittedText[:5])\n",
    "        ofile = open(ofilename,'a')\n",
    "        \n",
    "        for r in range(lenl):\n",
    "            linex = splittedText[r]\n",
    "            linex2 = \"\".join(c for c in linex if c not in ('!','.',':',',','?',';','``','&','-','\"','(',')','[',']','0','1','2','3','4','5','6','7','8','9'))\n",
    "            linex3 = linex2.split()\n",
    "            #prog=(r+1)/len(rawlines)\n",
    "            for s in range(len(linex3)):\n",
    "                noword = linex3[s].lower()\n",
    "                if noword not in self.swords:\n",
    "                    ofile.write(noword)\n",
    "                    ofile.write(\" \")\n",
    "\n",
    "# primary tf-idf class\n",
    "class flingTFIDF:\n",
    "    def __init__(self,data,cname):\n",
    "        self.idfMatrix = {}\n",
    "        self.distanceMatrix = {}\n",
    "        self.termsforIDF = []\n",
    "        self.cname = cname\n",
    "        self.data = data\n",
    "        self.lenv = len(self.data)\n",
    "        self.swords = set(stopwords.words('english'))\n",
    "\n",
    "    def drawProgressBar(self,percent, barLen = 50):\t\t\t#just a progress bar so that you dont lose patience\n",
    "        sys.stdout.write(\"\\r\")\n",
    "        progress = \"\"\n",
    "        for i in range(barLen):\n",
    "            if i<int(barLen * percent):\n",
    "                progress += \"=\"\n",
    "            else:\n",
    "                progress += \" \"\n",
    "        sys.stdout.write(\"[ %s ] %.2f%%\" % (progress, percent * 100))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def rem_stop_punct(self,originalText):\n",
    "        splittedText = originalText.split()\n",
    "        lenl = len(splittedText)\n",
    "        wordFiltered = []\n",
    "        tSent = []\n",
    "        for r in range(lenl):\n",
    "            wordx_1 = splittedText[r]\n",
    "            wordx_2 = \"\".join(c for c in wordx_1 if c not in ('!','.',':',',','?',';','``','&','-','\"','(',')','[',']','0','1','2','3','4','5','6','7','8','9')) \n",
    "            sWord = wordx_2.lower()\n",
    "            if sWord not in self.swords:\n",
    "                tSent.append(sWord)\n",
    "        return \" \".join(tSent)\n",
    "        \n",
    "    def smartTokenizeColumn(self):\n",
    "        self.stopsRemoved = []\n",
    "        for index, row in self.data.iterrows():\n",
    "            prog=(index+1)/self.lenv\n",
    "            originText = row[self.cname]\n",
    "            sentx = self.rem_stop_punct(originText)\n",
    "            self.drawProgressBar(prog)\n",
    "            self.data.loc[index,'stopsRemoved'] = sentx\n",
    "        self.cname = 'stopsRemoved'\n",
    "        \n",
    "    def getTF(self):\n",
    "        print(\"\\nAdding term frequency column based on\",self.cname)\n",
    "        tfMatrixList = []\n",
    "        for index, row in self.data.iterrows():\n",
    "            words_in_column = row[self.cname].split()\n",
    "            if len(words_in_column)!=0:\n",
    "                counts_all = Counter(words_in_column)\n",
    "                words, count_values = zip(*counts_all.items())\n",
    "                values_sorted, words_sorted = zip(*sorted(zip(count_values, words), key=operator.itemgetter(0), reverse=True))\n",
    "                tfMatrixList.append(pd.DataFrame({'word': words_sorted, 'tf': values_sorted}))\n",
    "                #self.data.loc[index,'tfMatrix'] = countdf\n",
    "            else:\n",
    "                #self.data.loc[index,'tfMatrix'] = pd.DataFrame(columns = ['word','tf'])\n",
    "                tfMatrixList.append(pd.DataFrame(columns = ['word','tf']))\n",
    "            prog=(index+1)/self.lenv\n",
    "            self.drawProgressBar(prog)\n",
    "        self.data['tfMatrix'] = tfMatrixList\n",
    "        \n",
    "    def getTFIDF(self):\n",
    "        print(\"\\nComputing and adding TF-IDF column based on\",self.cname)\n",
    "        for index, row in self.data.iterrows():\n",
    "            tfmatrixThisrow = row['tfMatrix']\n",
    "            tempTFIDF = []\n",
    "            for indx, rwx in tfmatrixThisrow.iterrows():\n",
    "                trmx = rwx['word']\n",
    "                tfx = rwx['tf']\n",
    "                idfx = self.idfMatrix[trmx]\n",
    "                tfidfx = tfx*idfx\n",
    "                tempTFIDF.append(tfidfx)\n",
    "                #tfmatrixThisrow.loc[index,'tf-idf'] = tfidfx\n",
    "            tfmatrixThisrow['tf-idf'] = tempTFIDF\n",
    "            #sumtfidf = tfmatrixThisrow['tf-idf'].sum() \n",
    "            prog=(index+1)/self.lenv\n",
    "            self.drawProgressBar(prog)\n",
    "                \n",
    "    def computeIDFlistofterms(self):\n",
    "        totalwords = 0\n",
    "        print(\"\\nComputing list of words for IDF...\\n\")\n",
    "        for index, row in self.data.iterrows():\n",
    "            words_in_column = set(row[self.cname].split())  \n",
    "            for word in words_in_column:\n",
    "                if word not in self.termsforIDF:\n",
    "                    self.termsforIDF.append(word)\n",
    "                    totalwords+=1\n",
    "        print(\"Created list of terms for IDF matrix with\", totalwords,\" terms.\")     \n",
    "        \n",
    "    def getIdf(self,term):\n",
    "        countPresentDocs = 0\n",
    "        lenidf = len(self.termsforIDF)\n",
    "        for i in range(lenidf):\n",
    "            tfx = self.getTermFreq(i,term)\n",
    "            if tfx>0:\n",
    "                countPresentDocs+=1\n",
    "            prog=(i+1)/lenidf\n",
    "            self.drawProgressBar(prog)\n",
    "        return countPresentDocs\n",
    "        \n",
    "    def computeIDFmatrix(self):\n",
    "        self.computeIDFlistofterms()\n",
    "        print(\"\\nComputing global IDF matrix...\\n\")\n",
    "        for term in self.termsforIDF:\n",
    "            self.idfMatrix[term]=0\n",
    "        for index, row in self.data.iterrows():\n",
    "            listofterms = list(self.data['tfMatrix'][index]['word'])\n",
    "            for term in listofterms:\n",
    "                self.idfMatrix[term]=self.idfMatrix[term]+1\n",
    "            prog=(index+1)/self.lenv\n",
    "            self.drawProgressBar(prog)\n",
    "        for term in self.termsforIDF:\n",
    "            idfx = self.idfMatrix[term]          \n",
    "            idfy = self.lenv/float(1+idfx)\n",
    "            idfz = math.log(idfy,10)\n",
    "            self.idfMatrix[term] = idfz\n",
    "            \n",
    "    def showData(self):\n",
    "        print(self.data['tfMatrix'])\n",
    "        \n",
    "    def createDistanceMetadata(self):\n",
    "        #sumList = []\n",
    "        for index, row in self.data.iterrows():\n",
    "            tfmatrixThisrow = row['tfMatrix']\n",
    "            sumTFIDF = tfmatrixThisrow['tf-idf'].sum()\n",
    "            #sumList.append({'sumTFIDF':sumTFIDF})\n",
    "            self.data.loc[index,'sumTFIDF'] = sumTFIDF\n",
    "            prog=(index+1)/self.lenv\n",
    "            self.drawProgressBar(prog)\n",
    "              \n",
    "    def distanceBtnTwoDocs(self, docId_1, docId_2):\n",
    "        listWords_1 = set(list(self.data['tfMatrix'][docId_1]['word']))\n",
    "        listWords_2 = set(list(self.data['tfMatrix'][docId_2]['word']))\n",
    "        common = listWords_1.intersection(listWords_2)\n",
    "        diff1_2 = listWords_1.difference(listWords_2)\n",
    "        diff2_1 = listWords_2.difference(listWords_1)\n",
    "        sumwt1 = self.data['sumTFIDF'][docId_1]\n",
    "        sumwt2 = self.data['sumTFIDF'][docId_2]\n",
    "        score_common, score_doc1, score_doc2 = 0,0,0\n",
    "        for word_c in common:\n",
    "            score_1 = float(self.data['tfMatrix'][docId_1].loc[self.data['tfMatrix'][docId_1]['word'] == word_c]['tf-idf'])\n",
    "            score_2 = float(self.data['tfMatrix'][docId_2].loc[self.data['tfMatrix'][docId_2]['word'] == word_c]['tf-idf'])\n",
    "            score_common += abs(score_1/float(sumwt1) - score_2/float(sumwt2))\n",
    "        for word_d12 in diff1_2:\n",
    "            score_1 = float(self.data['tfMatrix'][docId_1].loc[self.data['tfMatrix'][docId_1]['word'] == word_d12]['tf-idf'])\n",
    "            score_doc1 += score_1/float(sumwt1)\n",
    "        for word_d21 in diff2_1:\n",
    "            score_2 = float(self.data['tfMatrix'][docId_2].loc[self.data['tfMatrix'][docId_2]['word'] == word_d21]['tf-idf'])\n",
    "            score_doc2 += score_2/float(sumwt2)\n",
    "        score_total = score_common + score_doc1 + score_doc2\n",
    "        return(score_total)\n",
    "    \n",
    "    def computeDistanceBtnAllDocs(self):\n",
    "        for j in range(100):\n",
    "            for k in range(10):\n",
    "                numx = j*10+k\n",
    "                dist = self.distanceBtnTwoDocs(j,k)\n",
    "                self.distanceMatrix[(j,k)] = dist\n",
    "                prog=(numx+1)/1000\n",
    "                self.drawProgressBar(prog)\n",
    "                \n",
    "        print(self.distanceMatrix[:10])\n",
    "    \n",
    "    def writeToFile(self,fname):\n",
    "        self.data.to_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from imp import reload\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk,re,pprint\n",
    "import sys,glob,os\n",
    "import operator, string, argparse, math, random, statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "class flingPretrained:\n",
    "    '''\n",
    "    Trains linguistic models: doc2vec, fastText, word2vec, SDAE\n",
    "    Load pretrained linguistic models: doc2vec, fastText, word2vec, SDAE\n",
    "    Save group characteristics\n",
    "    \n",
    "    All embeddings available/level/word-order-preserval:\n",
    "        Glove : word / No\n",
    "        Doc2Vec : document / Yes\n",
    "        Word2Vec : word / No\n",
    "        TF-IDF : document / No\n",
    "        tfIDF weighted GloVe / No\n",
    "    '''\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.nDocs = len(self.data)\n",
    "        self.nDocsTest = 0\n",
    "        self.allDistances = {}\n",
    "        self.groupedCharacteristic = {'glove' : None, 'vec_tfidf-doc2vec' : None, 'vec_tfidf-glove' : None, 'doc2vec' : None}\n",
    "        self.wordVecModel = {'glove':None, 'doc2vec':None}\n",
    "        print(\"\\nWorking on pretrained word embeddings!\\n\")\n",
    "        \n",
    "    '''\n",
    "    Load pretrained word vectors: gloVe, fastText, doc2vec, word2vec, SDAE\n",
    "    by calling the appropriate load function for the vector type.\n",
    "    '''\n",
    "    def loadPretrainedWordVectors(self,vecType):\n",
    "        if vecType == 'glove':\n",
    "            self.wordVecModel['glove'] = self.loadGloveModel()\n",
    "            print(\"GloVe Vectors Loaded!\\n\") \n",
    "\n",
    "    '''\n",
    "    Loads the glove model provided a filename.\n",
    "    TASK: edit the function to take a filename instead of hard-coding the location of the GloVe model.\n",
    "    '''\n",
    "    def loadGloveModel(self):\n",
    "        print(\"Loading Glove Model\\n\")\n",
    "        try:\n",
    "            f = open('../datasets/glove.6B/glove.6B.50d.txt','r')\n",
    "        except:\n",
    "            f = open('datasets/glove.6B/glove.6B.50d.txt','r')\n",
    "        gloveModel = {}\n",
    "        for line in f:\n",
    "            splitLines = line.split()\n",
    "            word = splitLines[0]\n",
    "            wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "            gloveModel[word] = wordEmbedding\n",
    "        print(len(gloveModel),\" words loaded!\\n\")\n",
    "        return(gloveModel)\n",
    "    \n",
    "    '''\n",
    "    Returns the computed GloVe vector for the document. Note: a document contains multiple words, \n",
    "    and we have word vectors corresponding to every word in Glove\n",
    "    '''\n",
    "    def getDocVector(self,doc_Id):\n",
    "        gvl=self.getGloveVectorList(listx)\n",
    "        glove_dv = np.mean(gvl,axis=0)\n",
    "        return(glove_dv)\n",
    "    \n",
    "    '''\n",
    "    Returns a list of GloVe vectors for all words in the document.\n",
    "    '''\n",
    "    def getGloveVectorList(self,listx):\n",
    "        vecList = []\n",
    "        nf = []\n",
    "        presenceBit = []\n",
    "        for w in listx:\n",
    "            try:\n",
    "                vecList.append(self.wordVecModel['glove'][w])\n",
    "                presenceBit.append(1)\n",
    "            except:\n",
    "                presenceBit.append(0)\n",
    "                nf.append(w)\n",
    "                continue        \n",
    "        if len(vecList)==0:\n",
    "            return([[0]*50],[])\n",
    "        vecArray = np.stack(vecList, axis=0)\n",
    "        return vecArray,presenceBit  \n",
    "    \n",
    "    '''\n",
    "    Add two new computed vectors to the data.\n",
    "        a) glove-vector : plain GloVe vectors non-weighted\n",
    "        b) glove-tfidf : GloVe vectors weighted with their tfIDF scores \n",
    "    uses numpy.average(a, axis=None, weights=None, returned=False)[source]\n",
    "    '''\n",
    "    def addDocumentGloveVectors(self):\n",
    "        vecL = []\n",
    "        vecWL = []\n",
    "        for indx in range(self.nDocs):\n",
    "            listWords_1 = set(list(self.data['tfMatrix'][int(indx)]['word']))\n",
    "            tFreqs = np.asarray(list(self.data['tfMatrix'][int(indx)]['tf']))\n",
    "            gvl,prBit = self.getGloveVectorList(listWords_1)\n",
    "            if prBit == []:\n",
    "                vecL.append([0]*50)\n",
    "                vecWL.append([0]*50)         \n",
    "                continue;\n",
    "            termFreqs = [a*b for (a,b) in zip(prBit,tFreqs) if a*b!=0]            #print(\"listWords1,termFreqs\",listWords_1,termFreqs)\n",
    "            vecL.append(np.nanmean(gvl,axis=0))\n",
    "            vecWL.append(np.average(gvl, axis=0, weights=termFreqs))          \n",
    "        self.data['glove-vector'] = vecL\n",
    "        self.getDistanceDistribution(100,'glove-vector')\n",
    "        self.data['glove-tfIDF'] = vecWL\n",
    "        self.getDistanceDistribution(100,'glove-tfIDF')\n",
    "        \n",
    "    '''\n",
    "    Distance between two documents using TF-IDF dictionaries.\n",
    "        Method used: Using 'percentage of importance' by using tf-idf score as weights\n",
    "    '''\n",
    "    def distanceBtnTwoDocs(self, docId_1, docId_2):\n",
    "        listWords_1 = set(list(self.data['tfMatrix'][int(docId_1)]['word']))\n",
    "        listWords_2 = set(list(self.data['tfMatrix'][int(docId_2)]['word']))\n",
    "        common = listWords_1.intersection(listWords_2)\n",
    "        diff1_2 = listWords_1.difference(listWords_2)\n",
    "        diff2_1 = listWords_2.difference(listWords_1)\n",
    "        sumwt1 = self.data['sumTFIDF'][docId_1]\n",
    "        sumwt2 = self.data['sumTFIDF'][docId_2]\n",
    "        score_common, score_doc1, score_doc2 = 0,0,0\n",
    "        #print(len(common),len(diff1_2),len(diff2_1))\n",
    "        for word_c in common:\n",
    "            score_1 = float(self.data['tfMatrix'][docId_1].loc[self.data['tfMatrix'][docId_1]['word'] == word_c]['tf-idf'])\n",
    "            score_2 = float(self.data['tfMatrix'][docId_2].loc[self.data['tfMatrix'][docId_2]['word'] == word_c]['tf-idf'])\n",
    "            score_common += abs(score_1/float(sumwt1) - score_2/float(sumwt2))\n",
    "        for word_d12 in diff1_2:\n",
    "            score_1 = float(self.data['tfMatrix'][docId_1].loc[self.data['tfMatrix'][docId_1]['word'] == word_d12]['tf-idf'])\n",
    "            score_doc1 += score_1/float(sumwt1)\n",
    "        for word_d21 in diff2_1:\n",
    "            score_2 = float(self.data['tfMatrix'][docId_2].loc[self.data['tfMatrix'][docId_2]['word'] == word_d21]['tf-idf'])\n",
    "            score_doc2 += score_2/float(sumwt2)\n",
    "        score_total = score_common + score_doc1 + score_doc2\n",
    "        return(score_total)\n",
    "    \n",
    "    #document vector is the average of all the word vectors gloVe\n",
    "    def getDocVector(self,listx):\n",
    "        gvl=self.getGloveVectorList(listx)\n",
    "        glove_dv = np.mean(gvl,axis=0)\n",
    "        return(glove_dv)\n",
    "    \n",
    "    '''\n",
    "    Returns the distance between two GloVe vectors.\n",
    "    '''\n",
    "    def getGloveDistance(self,docId_1,docId_2,method):\n",
    "        listWords_1 = set(list(self.data['tfMatrix'].iloc[int(docId_1)]['word']))\n",
    "        listWords_2 = set(list(self.data['tfMatrix'].iloc[int(docId_2)]['word']))\n",
    "        if method == 'average':\n",
    "            dv_1 = self.getDocVector(listWords_1)\n",
    "            dv_2 = self.getDocVector(listWords_2)\n",
    "            dist = np.linalg.norm(dv_1-dv_2)\n",
    "            return dist\n",
    "              \n",
    "    def drawProgressBar(self, percent, barLen = 50):\t\t\t#just a progress bar so that you dont lose patience\n",
    "        sys.stdout.write(\"\\r\")\n",
    "        progress = \"\"\n",
    "        for i in range(barLen):\n",
    "            if i<int(barLen * percent):\n",
    "                progress += \"=\"\n",
    "            else:\n",
    "                progress += \" \"\n",
    "        sys.stdout.write(\"[ %s ] %.2f%%\" % (progress, percent * 100))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    def getDistance(self,docId_1,docId_2,vectorName):\n",
    "        if method == 'glove':\n",
    "            dv_1 = self.data['glove-vector'][int(docId_1)]\n",
    "            dv_2 = self.data['glove-vector'][int(docId_2)]\n",
    "        elif method == 'tfidf':\n",
    "            dv_1 = self.data['tfidf2vec-tfidf'][int(docId_1)]\n",
    "            dv_2 = self.data['tfidf2vec-tfidf'][int(docId_2)]           \n",
    "        dist = np.linalg.norm(dv_1-dv_2)\n",
    "        return dist\n",
    "\n",
    "    '''\n",
    "    Get sample distance distribution between numx random documents in the data and plot histogram \n",
    "    '''\n",
    "    def getDistanceDistribution(self,numx,vectorName):\n",
    "        numHalf = int(numx/2)\n",
    "        doca,docb = [],[]\n",
    "        for i in range(numHalf):\n",
    "            doca.append(random.randint(1,1026))\n",
    "            docb.append(random.randint(1027,2053))\n",
    "        distanceSample = []\n",
    "        total = numHalf*numHalf\n",
    "        for doc_1 in range(len(doca)):\n",
    "            for doc_2 in range(len(docb)):\n",
    "                dv_1 = self.data[vectorName][int(doc_1)]\n",
    "                dv_2 = self.data[vectorName][int(doc_2)]           \n",
    "                dist = np.linalg.norm(dv_1-dv_2)\n",
    "                distanceSample.append(dist)\n",
    "                cov = doc_1*numHalf + doc_2\n",
    "                prog=(cov+1)/total\n",
    "                self.drawProgressBar(prog)\n",
    "        pltx = plt.hist(distanceSample,bins=50)\n",
    "        return(pltx)\n",
    "    \n",
    "    '''\n",
    "    Returns the gloVe vector for the word from the pre-trained gloVe vectors.\n",
    "    '''\n",
    "    def getGloveScore(self,w):\n",
    "        try:\n",
    "            return(self.wordVecModel['glove'][w])\n",
    "        except:\n",
    "            return([0*50]) \n",
    "    \n",
    "    '''\n",
    "    Combines document tfIDF dictionary with other document vectors to create combined vectors. \n",
    "    '''\n",
    "    def doctfidf2vec(self,docId,mode):\n",
    "        docVecList = []\n",
    "        listWords = list(self.data['tfMatrix'][int(docId)]['word'])\n",
    "        if mode == \"tf-only\":\n",
    "            scores = list(self.data['tfMatrix'][int(docId)]['tf'])\n",
    "        elif mode == \"tf-idf\":\n",
    "            scores = list(self.data['tfMatrix'][int(docId)]['tf-idf'])\n",
    "        lenW =len(listWords)\n",
    "        gloveScores = [self.getGloveScore(el) for el in listWords]\n",
    "        for j in range(lenW):\n",
    "            temp = [float(scores[j])]*50\n",
    "            #gloveScores[j]\n",
    "            res = [a*b for (a,b) in zip(temp,gloveScores[j])]\n",
    "            if len(res)==1:\n",
    "                continue;\n",
    "            else:\n",
    "                docVecList.append(res)            \n",
    "        return(np.nanmean(docVecList,axis=0))\n",
    "    \n",
    "    '''\n",
    "    For each group in the specified column, average all the document vectors in the \n",
    "    group to create a group characteristic\n",
    "    \n",
    "    TASK: explore more options of averaging the vectors. '''\n",
    "    def createGroupedCharacteristics(self,column):\n",
    "        vecList = ['glove-vector','doc2vec','vec_tfidf-glove','glove-tfIDF']\n",
    "        self.dataTrain.groupby([column])\n",
    "        print(\"\\nComputing groupCharacteristics for,\",column)\n",
    "        for vec in vecList:\n",
    "            self.groupedCharacteristic[vec] = self.dataTrain.groupby(column)[vec].apply(np.average).to_frame()\n",
    " \n",
    "    '''\n",
    "    Function to return the group most simimar to the vector, based on distance computed with every group characteristics.\n",
    "    '''\n",
    "    def getNearestGroup(self,vec,vectorName):\n",
    "        minDist = math.inf\n",
    "        minGroup = None\n",
    "        for colx in fdb.groupedCharacteristic[vectorName].index.values:\n",
    "            vecy = fdb.groupedCharacteristic[vectorName].loc[colx].to_numpy(dtype=object)\n",
    "            if not np.all(vec):\n",
    "                vec = ([0.0001]*50)\n",
    "            distx = np.linalg.norm(scipy.spatial.distance.cosine(vec,vecy))\n",
    "            if distx < minDist:\n",
    "                minDist = distx\n",
    "                minGroup = colx                 \n",
    "        return minGroup\n",
    "    \n",
    "    '''\n",
    "    Explore options to optimize space using function.\n",
    "    '''\n",
    "    def splitTestTrain(self):\n",
    "        mPt = int(self.nDocs*0.7)\n",
    "        self.dataTrain = self.data[:mPt]\n",
    "        self.dataTest = self.data[mPt:]\n",
    "        self.nDocsTest = len(self.dataTest)\n",
    "               \n",
    "    '''\n",
    "    Add computed group as a new column.\n",
    "    '''\n",
    "    def addVectorComputedGroup(self,vectorName,groupName):\n",
    "        computedGroups = []\n",
    "        for docId in range(self.nDocsTest):\n",
    "            computedGroup = self.getNearestGroup(self.dataTest[vectorName].iloc[docId],vectorName)\n",
    "            computedGroups.append(computedGroup)           \n",
    "        self.dataTest[groupName] = computedGroups      \n",
    "    '''\n",
    "    Simple percentage count of documents which got the correct labels assigned.\n",
    "    '''  \n",
    "    def getAccuracy(self,compareWith,vecName):\n",
    "        countCorrect = 0\n",
    "        for d in range(self.nDocsTest):\n",
    "            if self.dataTest[vecName].iloc[d] == self.dataTest[compareWith].iloc[d]:\n",
    "                countCorrect+=1\n",
    "        print(\"Accuracy of\",vecName,countCorrect/self.nDocsTest*100,\"%\")\n",
    "\n",
    "    '''\n",
    "    Convert tfIDF dictionary for every document with precomputed word-embeddings\n",
    "    '''\n",
    "    def tfidf2vec(self,mode,method):\n",
    "        vecL = []\n",
    "        if mode == 'tf-only':\n",
    "            columnName = 'vec_tf-' + method\n",
    "            print(\"\\nComputing column:\",columnName)\n",
    "            for indx in range(self.nDocs):\n",
    "                gvl=self.doctfidf2vec(indx,'tf-only')\n",
    "                vecL.append(gvl)\n",
    "                prog=(indx+1)/self.nDocs\n",
    "                self.drawProgressBar(prog)\n",
    "        else:\n",
    "            columnName = 'vec_tfidf-' + method\n",
    "            print(\"\\nComputing column:\",columnName)\n",
    "            for indx in range(self.nDocs):\n",
    "                gvl=self.doctfidf2vec(indx,'tf-idf')\n",
    "                vecL.append(gvl)\n",
    "                prog=(indx+1)/self.nDocs\n",
    "                self.drawProgressBar(prog)\n",
    "        self.data[columnName] = vecL\n",
    "        try:\n",
    "            self.getDistanceDistribution(100,'glove-tfIDF')\n",
    "        except:\n",
    "            return\n",
    "\n",
    "class vectorize:\n",
    "    def __init__(self,data,factorName):\n",
    "        self.data = data\n",
    "        self.dataNew = []\n",
    "        self.model = None\n",
    "        self.swords = set(stopwords.words('english'))\n",
    "        self.factorName = factorName\n",
    "        for docId in range(len(self.data)):\n",
    "            dv_1 = self.data[factorName][int(docId)]\n",
    "            self.dataNew.append(dv_1)\n",
    "        self.nDocs = len(self.dataNew)\n",
    "        print(self.nDocs,\"documents added!\")\n",
    "        \n",
    "    '''\n",
    "    Tokenizer: Remove stopwords and punctuations.\n",
    "    TASK: Add standard available tokenizers.\n",
    "    '''\n",
    "    def rem_stop_punct(self,originalText):\n",
    "        splittedText = originalText.split()\n",
    "        lenl = len(splittedText)\n",
    "        wordFiltered = []\n",
    "        tSent = []\n",
    "        for r in range(lenl):\n",
    "            wordx_1 = splittedText[r]\n",
    "            wordx_2 = \"\".join(c for c in wordx_1 if c not in ('!','.',':',',','?',';','``','&','-','\"','(',')','[',']','0','1','2','3','4','5','6','7','8','9')) \n",
    "            sWord = wordx_2.lower()\n",
    "            if sWord not in self.swords:\n",
    "                tSent.append(sWord)\n",
    "        return tSent\n",
    "\n",
    "    def tagged_document(self,list_of_list_of_words):\n",
    "        for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "            yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "    '''\n",
    "    Train doc2vec vectors on the training dataset.\n",
    "    '''\n",
    "    def trainDocVectors(self):\n",
    "        print(\"\\nTraining doc2vec model.\")\n",
    "        self.data_for_training = list(self.tagged_document(self.dataNew))\n",
    "        self.model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=30)\n",
    "        self.model.build_vocab(self.data_for_training)\n",
    "        self.model.train(self.data_for_training, total_examples=self.model.corpus_count, epochs=self.model.epochs)\n",
    "        #self.model.to_pickle(\"model_doc2vec.pkl\")\n",
    "        return(self.model)\n",
    "        \n",
    "    def addDocVectors(self):\n",
    "        print(\"\\nAdding doc2vec vectors to dataset.\")\n",
    "        docVectors = []\n",
    "        for docId in range(len(self.data)):\n",
    "            docVectors.append(self.model.infer_vector(self.rem_stop_punct(self.data[self.factorName][int(docId)])))\n",
    "        self.data['doc2vec'] = docVectors\n",
    "        \n",
    "class flingDBSCAN:\n",
    "    def __init__(self,data,epsilon,minPts,method):\n",
    "        self.data = data\n",
    "        self.method = method\n",
    "        self.minPts = minPts\n",
    "        self.noisePts = []\n",
    "        self.nDocs = len(self.data)\n",
    "        self.clusterCharacteristic = None \n",
    "        self.clusterIndex = 0 \n",
    "        self.clusterCount = 0 \n",
    "        self.clusterLabel = \"computedCluster\"\n",
    "        print(\"\\nflingDBSCAN initialized!\\n\")\n",
    "        self.clusterMetadata = {}\n",
    "        for i in range(self.nDocs):\n",
    "            self.clusterMetadata[i] = None\n",
    "        if epsilon:\n",
    "            self.epsilon = epsilon\n",
    "        else:\n",
    "            if method == 'glove':\n",
    "                self.epsilon = self.getBestDistance('glove')\n",
    "                print(\"\\nBest epsilon computed on GLOVE =\",self.epsilon,\"\\n\")\n",
    "            else:\n",
    "                self.epsilon = self.getBestDistance('tfidf')\n",
    "                print(\"\\nBest epsilon computed on GLOVE-TFIDF =\",self.epsilon,\"\\n\")\n",
    "            \n",
    "    def getBestDistance(self,method):\n",
    "        numx = 100\n",
    "        numHalf = int(numx/2)\n",
    "        doca,docb = [],[]\n",
    "        print(\"computing best distance\")\n",
    "        for i in range(numHalf):\n",
    "            doca.append(random.randint(1,int(self.nDocs/2)))\n",
    "            docb.append(random.randint(int(self.nDocs/2)+1,self.nDocs))\n",
    "        distanceSample = []\n",
    "        total = numHalf*numHalf\n",
    "        for doc_1 in range(len(doca)):\n",
    "            for doc_2 in range(len(docb)):\n",
    "                if method == 'glove':\n",
    "                    distanceSample.append(self.getDistance(doc_1,doc_2,'glove'))\n",
    "                else:\n",
    "                    distanceSample.append(self.getDistance(doc_1,doc_2,'tfidf'))\n",
    "                cov = doc_1*numHalf + doc_2\n",
    "                prog=(cov+1)/total\n",
    "                self.drawProgressBar(prog)\n",
    "        plt.show(plt.hist(distanceSample,bins=20))\n",
    "        return statistics.mean(distanceSample)\n",
    "            \n",
    "    def assignLabel(self,dictDist,label):\n",
    "        for el in dictDist:\n",
    "            self.clusterMetadata[el]=label\n",
    "            \n",
    "    def printClusterInfo(self):\n",
    "        print(\"Cluster characteristics:\")\n",
    "        print(\" -- vectors:\",self.method)\n",
    "        print(\" -- minPts:\",self.minPts)\n",
    "        print(\" -- EstimatedBestDistance\",self.epsilon)\n",
    "        print(\" --\",self.clusterCount,\"clusters formed!\")\n",
    "        print(\" --\",self.nDocs-len(self.noisePts),\"points assigned to clusters!\") \n",
    "        print(\" --\",len(self.noisePts),\"noise points!\\n\")\n",
    "        noisePc = len(self.noisePts)/self.nDocs*100\n",
    "        print(\" --\",noisePc,\"% noise!\\n\")\n",
    "            \n",
    "    def printClusterMetadata(self,n):\n",
    "        for j in range(n):\n",
    "            print(j, self.clusterMetadata[j])\n",
    "         \n",
    "    # range query equivalent function\n",
    "    def findNeighborOf(self,ptIndex,method):\n",
    "        distance = {}      \n",
    "        #first vector\n",
    "        if method == 'glove':\n",
    "            dv_1 = self.data['glove-vector'][int(ptIndex)] \n",
    "        elif method == 'tfidf':\n",
    "            dv_1 = self.data['tfidf2vec-tfidf'][int(ptIndex)]\n",
    "        \n",
    "        #iterating over the whole data for the second vector \n",
    "        if method == 'tfidf':\n",
    "            for j in range(self.nDocs):\n",
    "                dv_2 = self.data['tfidf2vec-tfidf'][j]\n",
    "                if j!=ptIndex:\n",
    "                    distx = self.getDistance(ptIndex,j,'tfidf')\n",
    "                    distance[j] = distx\n",
    "        elif method == 'glove':\n",
    "            for j in range(self.nDocs):\n",
    "                dv_2 = self.data['glove-vector'][j]\n",
    "                if j!=ptIndex:\n",
    "                    distx = self.getDistance(ptIndex,j,'glove')\n",
    "                    distance[j] = distx\n",
    "        \n",
    "        # keeping only elements at a distnce of less than epsilon\n",
    "        tempDistances = {key:value for (key,value) in distance.items() if value<self.epsilon}\n",
    "        newDistances = {key:value for (key,value) in tempDistances.items() if self.clusterMetadata[key]==None}\n",
    "        # keeping the cluster only if we \n",
    "        if len(newDistances)>self.minPts:    \n",
    "            return newDistances.keys()\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    def dbscanCompute(self):\n",
    "        print(\"\\ninitiating DBSCAN Clustering with\",self.method,\"vectors\\n\")\n",
    "        self.clusterMetadata[0]='cluster_0_'\n",
    "        for k in range(self.nDocs):\n",
    "            if not self.clusterMetadata[k]:\n",
    "                if self.method=='glove':\n",
    "                    neighbors = self.findNeighborOf(k,'glove')\n",
    "                else:\n",
    "                    neighbors = self.findNeighborOf(k,'tfidf')\n",
    "                if neighbors:\n",
    "                    self.clusterCount+=1\n",
    "                    clusterName = \"cluster_\" + str(self.clusterCount)+\"_\"\n",
    "                    self.clusterMetadata[k] = clusterName\n",
    "                    \n",
    "                    # neighboring points of original point\n",
    "                    for nbPoint in neighbors:\n",
    "                        if not self.clusterMetadata[nbPoint]:\n",
    "                            self.clusterMetadata[nbPoint] = clusterName\n",
    "                    if self.method=='glove':\n",
    "                        innerNeighbors = self.findNeighborOf(k,'glove')\n",
    "                    else:\n",
    "                        innerNeighbors = self.findNeighborOf(k,'tfidf')\n",
    "                    if innerNeighbors:\n",
    "                        for nb in innerNeighbors:\n",
    "                            self.clusterMetadata[nb] = clusterName\n",
    "                            neighbors.append(nb)                          \n",
    "                    print(\"\\n ---- \",clusterName,\"assigned to\",len(neighbors),\"points! ----\")\n",
    "                else:\n",
    "                    self.noisePts.append(k)\n",
    "            prog=(k+1)/self.nDocs\n",
    "            self.drawProgressBar(prog)\n",
    "        print(\"\\n\",self.clusterCount,\"clusters formed!\")\n",
    "\n",
    "    ''' \n",
    "    Get distance between two vectors based on method. Only single vector methods available. Combined methods yet to add.\n",
    "    '''\n",
    "    def getDistance(self,docId_1,docId_2,method):\n",
    "        if method == 'glove':\n",
    "            dv_1 = self.data['glove-vector'][int(docId_1)]\n",
    "            dv_2 = self.data['glove-vector'][int(docId_2)]\n",
    "        elif method == 'tfidf':\n",
    "            dv_1 = self.data['tfidf2vec-tfidf'][int(docId_1)]\n",
    "            dv_2 = self.data['tfidf2vec-tfidf'][int(docId_2)]           \n",
    "        dist = np.linalg.norm(dv_1-dv_2)\n",
    "        return dist\n",
    "    \n",
    "    def addClusterLabel(self,label):\n",
    "        self.clusterLabel = label\n",
    "        vec = []\n",
    "        for el in self.clusterMetadata.keys():\n",
    "            vec.append(self.clusterMetadata[el])\n",
    "        self.data[label] = vec\n",
    "        \n",
    "    def getNearestGroup(self,vec):\n",
    "        minDist = 100\n",
    "        minGroup = None\n",
    "        for colx in fdb.groupedCharacteristic.columns:\n",
    "            vecy = fdb.groupedCharacteristic[colx]['glove-vector']\n",
    "            distx = np.linalg.norm(vec-vecy)\n",
    "            if distx<minDist:\n",
    "                minDist = distx\n",
    "                minGroup = colx\n",
    "        return minGroup\n",
    "    \n",
    "    def addClusterMajorityLabel(self):\n",
    "        clusterMap = {}\n",
    "        for docId in range(self.nDocs):\n",
    "            computedGroup = self.getNearestGroup(self.data['glove-vector'][int(docId)])\n",
    "            clID = self.data['glove-vector'][int(docId)]\n",
    "            if clID not in self.clusterCharacteristic:\n",
    "                self.clusterCharacteristic[clID]=[computedGroup]\n",
    "            else:\n",
    "                self.clusterCharacteristic[clID].append(computedGroup)\n",
    "            prog=(docId+1)/self.nDocs\n",
    "            self.drawProgressBar(prog)\n",
    "        for k in self.clusterCharacteristic.key():\n",
    "            res = statistics.mode(self.clusterCharacteristic[k])\n",
    "            clusterMap[k] = res\n",
    "        return(clusterMap)\n",
    "\n",
    "    def addVectorComputedGroup(self,vecName,factorName):\n",
    "        computedGroups = []\n",
    "        for docId in range(self.nDocs):\n",
    "            computedGroup = self.getNearestGroup(self.data[vecName][int(docId)])\n",
    "            computedGroups.append(computedGroup)\n",
    "        self.data[factorName] = computedGroups\n",
    "              \n",
    "    def getAccuracy(self,compareWith):\n",
    "        countCorrect = 0\n",
    "        for d in range(self.nDocs):\n",
    "            if self.data['characteristicGroup'][d] == self.data[compareWith][d]:\n",
    "                countCorrect+=1\n",
    "        print(\"Accuracy:\",countCorrect/self.nDocs*100,\"%\")\n",
    "        \n",
    "    def evaluateClusterPerformance(self,compareWith):\n",
    "        return(metrics.adjusted_rand_score(self.data['characteristicGroup'],self.data[compareWith]))\n",
    "    \n",
    "    '''\n",
    "    TASK: optimize the progressbar.\n",
    "    '''\n",
    "    def drawProgressBar(self, percent, barLen = 50):\t\t\t#just a progress bar so that you dont lose patience\n",
    "        sys.stdout.write(\"\\r\")\n",
    "        progress = \"\"\n",
    "        for i in range(barLen):\n",
    "            if i<int(barLen * percent):\n",
    "                progress += \"=\"\n",
    "            else:\n",
    "                progress += \" \"\n",
    "        sys.stdout.write(\"[ %s ] %.2f%%\" % (progress, percent * 100))\n",
    "        sys.stdout.flush()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fling import utilities as ut\n",
    "#from fling import tfidfModule as tfm\n",
    "os.chdir(\"/Users/arnabborah/Documents/repositories/fling/\")\n",
    "spamtm = dataProcessor(\"datasets/spamTextMessages.csv\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary distance module run\n",
    "ftf = flingTFIDF(spamtm.dataInitial,'Message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ================================================== ] 100.00%\n",
      "Adding term frequency column based on stopsRemoved\n",
      "[ ================================================== ] 100.00%\n",
      "Computing list of words for IDF...\n",
      "\n",
      "Created list of terms for IDF matrix with 8780  terms.\n",
      "\n",
      "Computing global IDF matrix...\n",
      "\n",
      "[ ================================================== ] 100.00%\n",
      "Computing and adding TF-IDF column based on stopsRemoved\n",
      "[ ================================================== ] 100.00%"
     ]
    }
   ],
   "source": [
    "ftf.smartTokenizeColumn()\n",
    "ftf.getTF()\n",
    "ftf.computeIDFmatrix()\n",
    "ftf.getTFIDF()\n",
    "ftf.createDistanceMetadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572 documents added!\n",
      "\n",
      "Training doc2vec model.\n",
      "\n",
      "Adding doc2vec vectors to dataset.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "#training and adding doc2vec vectors based on column 'Messages'\n",
    "vecc = vectorize(ftf.data,'Message')\n",
    "trained_doc2vec_model = vecc.trainDocVectors()\n",
    "vecc.addDocVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on pretrained word embeddings!\n",
      "\n",
      "Loading Glove Model\n",
      "\n",
      "400000  words loaded!\n",
      "\n",
      "GloVe Vectors Loaded!\n",
      "\n",
      "[ ================================================== ] 100.00%\n",
      "Computing column: vec_tfidf-glove\n",
      "[ ==                                                 ] 5.65%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:237: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ================================================== ] 100.00%[ ============================================       ] 88.00%\n",
      "Computing groupCharacteristics for, Category\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of cGroup_glove 79.84449760765551 %\n",
      "Accuracy of cGroup_doc2vec 56.10047846889952 %\n",
      "Accuracy of cGroup_gloveWt_tfidf 79.72488038277513 %\n",
      "Accuracy of cGroup_tfidf-glove 0.0 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adding pretrained and combined vectors and vector computed group\n",
    "import gensim \n",
    "import tfidfModule as tfm\n",
    "\n",
    "#dataProcessed = pd.read_pickle('datasets/data_tfidf_processed.pkl')\n",
    "fdb = tfm.flingPretrained(ftf.data)\n",
    "#adding pretrained glove vectors \n",
    "fdb.loadPretrainedWordVectors('glove')\n",
    "fdb.addDocumentGloveVectors()\n",
    "\n",
    "#adding combo vectors with tfidf and (glove + doc2vec) for inter sentence semantic information addition\n",
    "fdb.tfidf2vec('tf-idf','glove')\n",
    "#fdb.tfidf2vec('tf-idf','doc2vec')\n",
    "fdb.splitTestTrain()\n",
    "\n",
    "# train group characteristics on column 'category' and predict vector based category, and compute error\n",
    "fdb.createGroupedCharacteristics('Category')\n",
    "fdb.addVectorComputedGroup('glove-vector','cGroup_glove')\n",
    "fdb.addVectorComputedGroup('doc2vec','cGroup_doc2vec')\n",
    "fdb.addVectorComputedGroup('glove-tfIDF','cGroup_gloveWt_tfidf')\n",
    "fdb.addVectorComputedGroup('vec_tfidf-glove','cGroup_tfidf-glove')\n",
    "\n",
    "#fdb.addVectorComputedGroup('vec_tfidf-doc2vec','cGroup_tfidf-doc2vec')\n",
    "fdb.getAccuracy('Category','cGroup_glove')\n",
    "fdb.getAccuracy('Category','cGroup_doc2vec')\n",
    "fdb.getAccuracy('Category','cGroup_gloveWt_tfidf')\n",
    "fdb.getAccuracy('Category','cGroup_tfidf-glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>stopsRemoved</th>\n",
       "      <th>tfMatrix</th>\n",
       "      <th>sumTFIDF</th>\n",
       "      <th>glove-vector</th>\n",
       "      <th>doc2vec</th>\n",
       "      <th>vec_tfidf-glove</th>\n",
       "      <th>glove-tfIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>word  tf    tf-idf\n",
       "0          go   1 ...</td>\n",
       "      <td>38.281443</td>\n",
       "      <td>[0.21390625000000005, 0.3857445625, -0.1334233...</td>\n",
       "      <td>[0.023361368, -0.01737977, -0.015450012, 0.030...</td>\n",
       "      <td>[0.5959587370342363, 0.8768489600169621, -0.51...</td>\n",
       "      <td>[0.21390625000000005, 0.3857445625, -0.1334233...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>word  tf    tf-idf\n",
       "0      ok   1  1.31950...</td>\n",
       "      <td>12.583182</td>\n",
       "      <td>[-0.34427266666666667, -0.11794016666666667, 0...</td>\n",
       "      <td>[0.01824778, -0.032706685, -0.040531408, 0.051...</td>\n",
       "      <td>[-0.6469232570438529, -0.4567087086249299, -0....</td>\n",
       "      <td>[-0.34427266666666667, -0.11794016666666667, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry  wkly comp win fa cup final tkts st...</td>\n",
       "      <td>word  tf    tf-idf\n",
       "0         entry ...</td>\n",
       "      <td>49.524838</td>\n",
       "      <td>[-0.3973114285714286, 0.43085399999999996, -0....</td>\n",
       "      <td>[0.0028259559, 0.0051632347, 0.0085555585, 0.0...</td>\n",
       "      <td>[-1.3248417366882783, 1.1032396880662387, -0.5...</td>\n",
       "      <td>[-0.38814812499999995, 0.482244125, -0.0930190...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>word  tf    tf-idf\n",
       "0        u   2  1.669...</td>\n",
       "      <td>16.431526</td>\n",
       "      <td>[0.17517428571428573, 0.24041571428571432, 0.2...</td>\n",
       "      <td>[-0.00799464, -0.02401544, -0.069796, 0.108990...</td>\n",
       "      <td>[0.5123484443079068, 0.5753741094651186, 0.311...</td>\n",
       "      <td>[0.19404333333333332, 0.1887144444444444, 0.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah think goes usf lives around though</td>\n",
       "      <td>word  tf    tf-idf\n",
       "0     nah   1  2.70461...</td>\n",
       "      <td>16.678825</td>\n",
       "      <td>[0.19229857142857143, 0.4842861428571427, 0.19...</td>\n",
       "      <td>[-0.005010659, 0.00058875955, 0.0060600745, -0...</td>\n",
       "      <td>[0.47549546283727345, 1.2971284798171407, 0.48...</td>\n",
       "      <td>[0.19229857142857143, 0.4842861428571427, 0.19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>nd time tried  contact u u  pound prize  cla...</td>\n",
       "      <td>word  tf    tf-idf\n",
       "0            ...</td>\n",
       "      <td>29.685673</td>\n",
       "      <td>[-0.0032673076923077126, 0.2802763076923076, 0...</td>\n",
       "      <td>[-0.024376733, -0.06455596, -0.054250874, 0.07...</td>\n",
       "      <td>[-0.018272115681935517, 0.38059669250977396, 0...</td>\n",
       "      <td>[-0.0284332142857143, 0.348728, 0.557019499999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will  b going to esplanade fr home?</td>\n",
       "      <td> b going esplanade fr home</td>\n",
       "      <td>word  tf    tf-idf\n",
       "0            1  1...</td>\n",
       "      <td>12.328684</td>\n",
       "      <td>[0.2450002, 0.433446, -0.009058, -0.1098192, 0...</td>\n",
       "      <td>[0.026062477, -0.083403476, 0.0068779653, 0.02...</td>\n",
       "      <td>[0.7474427145489011, 1.10828246067343, -0.1888...</td>\n",
       "      <td>[0.2450002, 0.433446, -0.009058, -0.1098192, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>pity * mood soany suggestions</td>\n",
       "      <td>word  tf    tf-idf\n",
       "0         pity   ...</td>\n",
       "      <td>15.080331</td>\n",
       "      <td>[-0.16283274999999997, 0.44291, -0.20726499999...</td>\n",
       "      <td>[0.2500398, 0.048694123, 0.01599926, 0.1155695...</td>\n",
       "      <td>[-0.10259994304988762, 1.0048869796671016, -0....</td>\n",
       "      <td>[-0.16283274999999997, 0.44291, -0.20726499999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>guy bitching acted like i'd interested buying ...</td>\n",
       "      <td>word  tf    tf-idf\n",
       "0          guy   ...</td>\n",
       "      <td>32.770129</td>\n",
       "      <td>[0.1625725923076923, -0.08889784615384616, 0.0...</td>\n",
       "      <td>[0.0029246681, -0.00042458618, 0.0025144704, -...</td>\n",
       "      <td>[0.4332410359621821, -0.43844977348643754, 0.1...</td>\n",
       "      <td>[0.1625725923076923, -0.08889784615384616, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>rofl true name</td>\n",
       "      <td>word  tf    tf-idf\n",
       "0  rofl   1  3.143951\n",
       "1 ...</td>\n",
       "      <td>7.558242</td>\n",
       "      <td>[0.264305, 0.7071000000000001, -0.36934, 0.107...</td>\n",
       "      <td>[0.0020055978, 0.006878876, -0.008461955, 0.00...</td>\n",
       "      <td>[0.5901408379733141, 1.5551135508332325, -0.80...</td>\n",
       "      <td>[0.264305, 0.7071000000000001, -0.36934, 0.107...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message  \\\n",
       "0         ham  Go until jurong point, crazy.. Available only ...   \n",
       "1         ham                      Ok lar... Joking wif u oni...   \n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3         ham  U dun say so early hor... U c already then say...   \n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "...       ...                                                ...   \n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5568      ham              Will  b going to esplanade fr home?   \n",
       "5569      ham  Pity, * was in mood for that. So...any other s...   \n",
       "5570      ham  The guy did some bitching but I acted like i'd...   \n",
       "5571      ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                           stopsRemoved  \\\n",
       "0     go jurong point crazy available bugis n great ...   \n",
       "1                               ok lar joking wif u oni   \n",
       "2     free entry  wkly comp win fa cup final tkts st...   \n",
       "3                   u dun say early hor u c already say   \n",
       "4                nah think goes usf lives around though   \n",
       "...                                                 ...   \n",
       "5567  nd time tried  contact u u  pound prize  cla...   \n",
       "5568                        b going esplanade fr home   \n",
       "5569                      pity * mood soany suggestions   \n",
       "5570  guy bitching acted like i'd interested buying ...   \n",
       "5571                                     rofl true name   \n",
       "\n",
       "                                               tfMatrix   sumTFIDF  \\\n",
       "0              word  tf    tf-idf\n",
       "0          go   1 ...  38.281443   \n",
       "1          word  tf    tf-idf\n",
       "0      ok   1  1.31950...  12.583182   \n",
       "2                word  tf    tf-idf\n",
       "0         entry ...  49.524838   \n",
       "3           word  tf    tf-idf\n",
       "0        u   2  1.669...  16.431526   \n",
       "4          word  tf    tf-idf\n",
       "0     nah   1  2.70461...  16.678825   \n",
       "...                                                 ...        ...   \n",
       "5567                word  tf    tf-idf\n",
       "0            ...  29.685673   \n",
       "5568          word  tf    tf-idf\n",
       "0            1  1...  12.328684   \n",
       "5569            word  tf    tf-idf\n",
       "0         pity   ...  15.080331   \n",
       "5570            word  tf    tf-idf\n",
       "0          guy   ...  32.770129   \n",
       "5571     word  tf    tf-idf\n",
       "0  rofl   1  3.143951\n",
       "1 ...   7.558242   \n",
       "\n",
       "                                           glove-vector  \\\n",
       "0     [0.21390625000000005, 0.3857445625, -0.1334233...   \n",
       "1     [-0.34427266666666667, -0.11794016666666667, 0...   \n",
       "2     [-0.3973114285714286, 0.43085399999999996, -0....   \n",
       "3     [0.17517428571428573, 0.24041571428571432, 0.2...   \n",
       "4     [0.19229857142857143, 0.4842861428571427, 0.19...   \n",
       "...                                                 ...   \n",
       "5567  [-0.0032673076923077126, 0.2802763076923076, 0...   \n",
       "5568  [0.2450002, 0.433446, -0.009058, -0.1098192, 0...   \n",
       "5569  [-0.16283274999999997, 0.44291, -0.20726499999...   \n",
       "5570  [0.1625725923076923, -0.08889784615384616, 0.0...   \n",
       "5571  [0.264305, 0.7071000000000001, -0.36934, 0.107...   \n",
       "\n",
       "                                                doc2vec  \\\n",
       "0     [0.023361368, -0.01737977, -0.015450012, 0.030...   \n",
       "1     [0.01824778, -0.032706685, -0.040531408, 0.051...   \n",
       "2     [0.0028259559, 0.0051632347, 0.0085555585, 0.0...   \n",
       "3     [-0.00799464, -0.02401544, -0.069796, 0.108990...   \n",
       "4     [-0.005010659, 0.00058875955, 0.0060600745, -0...   \n",
       "...                                                 ...   \n",
       "5567  [-0.024376733, -0.06455596, -0.054250874, 0.07...   \n",
       "5568  [0.026062477, -0.083403476, 0.0068779653, 0.02...   \n",
       "5569  [0.2500398, 0.048694123, 0.01599926, 0.1155695...   \n",
       "5570  [0.0029246681, -0.00042458618, 0.0025144704, -...   \n",
       "5571  [0.0020055978, 0.006878876, -0.008461955, 0.00...   \n",
       "\n",
       "                                        vec_tfidf-glove  \\\n",
       "0     [0.5959587370342363, 0.8768489600169621, -0.51...   \n",
       "1     [-0.6469232570438529, -0.4567087086249299, -0....   \n",
       "2     [-1.3248417366882783, 1.1032396880662387, -0.5...   \n",
       "3     [0.5123484443079068, 0.5753741094651186, 0.311...   \n",
       "4     [0.47549546283727345, 1.2971284798171407, 0.48...   \n",
       "...                                                 ...   \n",
       "5567  [-0.018272115681935517, 0.38059669250977396, 0...   \n",
       "5568  [0.7474427145489011, 1.10828246067343, -0.1888...   \n",
       "5569  [-0.10259994304988762, 1.0048869796671016, -0....   \n",
       "5570  [0.4332410359621821, -0.43844977348643754, 0.1...   \n",
       "5571  [0.5901408379733141, 1.5551135508332325, -0.80...   \n",
       "\n",
       "                                            glove-tfIDF  \n",
       "0     [0.21390625000000005, 0.3857445625, -0.1334233...  \n",
       "1     [-0.34427266666666667, -0.11794016666666667, 0...  \n",
       "2     [-0.38814812499999995, 0.482244125, -0.0930190...  \n",
       "3     [0.19404333333333332, 0.1887144444444444, 0.15...  \n",
       "4     [0.19229857142857143, 0.4842861428571427, 0.19...  \n",
       "...                                                 ...  \n",
       "5567  [-0.0284332142857143, 0.348728, 0.557019499999...  \n",
       "5568  [0.2450002, 0.433446, -0.009058, -0.1098192, 0...  \n",
       "5569  [-0.16283274999999997, 0.44291, -0.20726499999...  \n",
       "5570  [0.1625725923076923, -0.08889784615384616, 0.0...  \n",
       "5571  [0.264305, 0.7071000000000001, -0.36934, 0.107...  \n",
       "\n",
       "[5572 rows x 9 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdb.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               glove-vector\n",
      "Category                                                   \n",
      "ham       [0.08621057522946847, 0.16108873455431685, 0.1...\n",
      "spam      [0.03802002928660192, 0.25794960063990663, 0.2...\n",
      "                                                    doc2vec\n",
      "Category                                                   \n",
      "ham       [0.004500571, -0.0046128747, -0.0037903993, 0....\n",
      "spam      [0.007353711, 0.00052847125, -0.0015033685, 0....\n",
      "                                            vec_tfidf-glove\n",
      "Category                                                   \n",
      "ham       [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...\n",
      "spam      [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...\n",
      "                                                glove-tfIDF\n",
      "Category                                                   \n",
      "ham       [0.08530872151116166, 0.16057492748951366, 0.1...\n",
      "spam      [0.036856856000276404, 0.2598861828935243, 0.2...\n"
     ]
    }
   ],
   "source": [
    "# glove-TFIDF computing nan values as distances\n",
    "vecList = ['glove-vector','doc2vec','vec_tfidf-glove','glove-tfIDF']\n",
    "for vec in vecList:\n",
    "    print(fdb.dataTrain.groupby('Category')[vec].apply(np.average).to_frame())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
